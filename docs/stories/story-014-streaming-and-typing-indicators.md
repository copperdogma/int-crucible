# Story 014: Streaming architect responses and typing indicators

**Status**: To Do  

---

## Related Requirement
- See `docs/requirements.md`:
  - **Key Features – Interaction shell (MVP UI)** – should feel like a natural, responsive conversation.
- See `docs/design.md`:
  - **Feature: Chat-First Project & ProblemSpec Modelling** – conversational entry point.

## Alignment with Design
- This story builds on:
  - Story 012 (Architect-led conversational loop and logging).
  - Story 013 (Spec/world-model deltas and highlighting).
- It makes the Architect feel **alive and responsive** by:
  - Streaming responses as they are generated.
  - Showing a clear “typing/busy” indicator in the message stream and input area.
  - Providing robust error behavior that does not break the chat.

## Problem Statement
Without streaming and clear busy indicators:

1. Users see responses only after the model has finished generating, which can feel slow or frozen.
2. There is no explicit visual feedback that the Architect is “thinking”.
3. Long responses can feel jarring when they suddenly appear all at once.

The goal is to make the Architect behave like modern AI chat interfaces:
- Gradually streaming text.
- Showing a typing indicator.
- Keeping the UI responsive even for long replies.

## Acceptance Criteria
- **Backend: streaming responses**:
  - There is a streaming endpoint (e.g. via FastAPI `StreamingResponse` or Server-Sent Events) that:
    - Streams Architect reply content incrementally (token/word/chunk-based).
    - Emits enough metadata to allow the frontend to know when the message is complete.
  - If streaming fails mid-way:
    - The backend still makes best-effort to provide the completed response (e.g., via a final non-streaming fallback or a retry).
    - If the underlying Architect call completed successfully, the user still sees the full final reply, even if streaming itself failed.

- **Frontend: streaming consumption**:
  - ChatInterface consumes streaming Architect responses using:
    - `EventSource` (SSE) or a streaming `fetch`/`ReadableStream`.
  - While streaming:
    - A partial Architect message is rendered and updated live in the chat.
    - The message area auto-scrolls to follow the streaming text.
  - When streaming completes:
    - The partial message is finalized (e.g., marked as complete, no further updates).
    - The final content is consistent with what is logged in the database.

- **Typing / busy indicators**:
  - When the user sends a message and the Architect is generating a reply:
    - A clear **typing indicator bubble** appears in the chat (e.g., “Architect is thinking…” with animated dots).
    - The input area reflects the busy state (e.g., button text changes, subtle styling), but:
      - The user is not confused into thinking the UI is frozen.
  - The typing indicator:
    - Appears quickly (no long delay before showing).
    - Disappears reliably when the Architect response completes or fails.

- **Error handling**:
  - If streaming fails:
    - A system message is injected into the chat explaining that streaming failed.
    - If a fallback response is available, it is rendered as a normal Architect message.
  - Network or backend errors do not leave partial artifacts in the UI (e.g., stuck typing indicators, incomplete ghost messages).

- **Performance and responsiveness**:
  - Streaming must not introduce noticeable UI jank or freezing, even for long responses.
  - The chat input and scrolling remain responsive throughout streaming.
  - Auto-scroll behavior feels smooth and natural (not jumpy or erratic).

- **Logging consistency**:
  - Regardless of streaming behavior, the **final Architect message content** is:
    - Stored in `crucible_messages` as one coherent `agent` message.
    - Linked (via metadata) to any underlying refine or guidance operations.

## Tasks
- **Backend**:
  - [ ] Design a streaming response format for Architect replies:
    - Decide between SSE vs. streaming `StreamingResponse`.
    - Define message framing (chunks containing `text`, `done` flag, optional metadata).
  - [ ] Implement a streaming Architect endpoint that:
    - Uses the same guidance/Architect logic as Story 012.
    - Streams content as it is generated by the LLM.
    - Emits a clear completion signal.
  - [ ] Ensure that, once streaming finishes, the backend:
    - Persists the full reply as a single `crucible_messages` row for that chat session.

- **Frontend**:
  - [ ] Add a streaming client in `ChatInterface` (or a dedicated hook) that:
    - Opens a stream to the Architect endpoint.
    - Updates a “live” Architect message in the React state as chunks arrive.
  - [ ] Implement a typing/“thinking” indicator:
    - Rendered as a lightweight message bubble or row in the chat.
    - Controlled by Architect request state (pending/streaming vs. idle).
  - [ ] Ensure smooth auto-scroll behavior during streaming, without jitter.
  - [ ] Update the API client layer (e.g., `frontend/lib/api.ts` or a dedicated streaming client) to handle the chosen streaming protocol (SSE or streaming fetch) with appropriate TypeScript types.

- **UX & error behavior**:
  - [ ] Define and implement visual states for:
    - Streaming in progress.
    - Streaming success + completion.
    - Streaming failure (with a clear message).
  - [ ] Verify that:
    - The user always sees a clear "the system is working" state when waiting.
    - On failure, they get understandable feedback and the ability to retry or continue.

- **Browser testing and UI verification**:
  - [ ] **CRITICAL**: Use browser tools to test the implementation in the live UI:
    - Start the frontend and backend servers.
    - Navigate to the chat interface.
    - Send messages and verify:
      - Typing indicator appears quickly when Architect is generating a reply.
      - Architect responses stream in smoothly (token by token or chunk by chunk).
      - Auto-scroll follows the streaming text without jitter.
      - Input area shows appropriate busy state during streaming.
      - Streaming completes cleanly and final message is stored correctly.
      - Error handling works gracefully (test with network throttling if possible).
      - UI remains responsive during streaming (no freezing or jank).
    - Test with short, medium, and long responses to verify performance.
    - Verify the UI is elegant, functional, and matches the acceptance criteria.
    - Fix any issues found during browser testing before proceeding to sign-off.

- **Sign-off**:
  - [ ] Test streaming with:
    - Short, medium, and long Architect responses.
    - Both fast and slow network conditions (where possible).
  - [ ] Confirm that all final Architect responses appear in both:
    - The chat UI.
    - The database logs (`crucible_messages`) with consistent content.
  - [ ] User must sign off on streaming behavior and indicators before this story can be marked complete.


