# Int Crucible Configuration
# Copy this file to .env and fill in your values
#
# This file configures both Int Crucible and Kosmos (which provides LLM access).
# Int Crucible-specific settings are at the top, followed by LLM provider settings.

# ============================================================================
# INT CRUCIBLE CONFIGURATION
# ============================================================================

# Database URL (shared with Kosmos)
# SQLite (default, single file): sqlite:///crucible.db
# PostgreSQL (production): postgresql://user:password@host:port/database
DATABASE_URL=sqlite:///crucible.db

# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# API server configuration
API_HOST=127.0.0.1
API_PORT=8000

# ============================================================================
# LLM PROVIDER CONFIGURATION (Required for ProblemSpec Agent)
# ============================================================================
# These settings are used by Kosmos to provide LLM access to Int Crucible.
# You must configure at least one LLM provider for the ProblemSpec agent to work.

# Choose which LLM provider to use: "anthropic" (default) or "openai"
LLM_PROVIDER=anthropic

# ============================================================================
# ANTHROPIC/CLAUDE CONFIGURATION
# ============================================================================
# Required if LLM_PROVIDER=anthropic
# Get your API key from https://console.anthropic.com/

# Option 1: Use Anthropic API (Pay-per-use)
ANTHROPIC_API_KEY=sk-ant-api03-your-actual-key-here

# Option 2: Use Claude Code CLI (for Max subscription users)
# Set API key to all 9s to route to local Claude Code CLI
# Requires: pip install git+https://github.com/jimmc414/claude_n_codex_api_proxy.git
# Requires: claude CLI installed and authenticated
# ANTHROPIC_API_KEY=999999999999999999999999999999999999999999999999

# Claude model to use (only for API mode)
# Options: claude-3-5-sonnet-20241022, claude-3-opus-20240229, claude-3-5-haiku-20241022
CLAUDE_MODEL=claude-3-5-sonnet-20241022

# Maximum tokens per request
CLAUDE_MAX_TOKENS=4096

# Temperature for generation (0.0-1.0, higher = more creative)
CLAUDE_TEMPERATURE=0.7

# Enable prompt caching to reduce API costs (true/false)
CLAUDE_ENABLE_CACHE=true

# ============================================================================
# OPENAI CONFIGURATION
# ============================================================================
# Required if LLM_PROVIDER=openai
# Get your API key from https://platform.openai.com/api-keys

# OPENAI_API_KEY=sk-proj-your-actual-key-here

# Model to use
# Options: gpt-4-turbo, gpt-4, gpt-3.5-turbo, o1-preview, o1-mini
# OPENAI_MODEL=gpt-4-turbo

# Maximum tokens per request
# OPENAI_MAX_TOKENS=4096

# Temperature (0.0-2.0, higher = more creative)
# OPENAI_TEMPERATURE=0.7

# ============================================================================
# OPENAI-COMPATIBLE PROVIDERS (Ollama, OpenRouter, LM Studio, etc.)
# ============================================================================
# These use the OpenAI provider but connect to alternative services

# Example: Use Ollama locally (free, no API key needed)
# 1. Install Ollama: https://ollama.com/download
# 2. Start Ollama: ollama serve
# 3. Pull a model: ollama pull llama3.1:70b
# Then configure:
# LLM_PROVIDER=openai
# OPENAI_API_KEY=ollama
# OPENAI_BASE_URL=http://localhost:11434/v1
# OPENAI_MODEL=llama3.1:70b

# Example: Use OpenRouter (access to 100+ models)
# LLM_PROVIDER=openai
# OPENAI_API_KEY=sk-or-v1-your-key-here
# OPENAI_BASE_URL=https://openrouter.ai/api/v1
# OPENAI_MODEL=anthropic/claude-3.5-sonnet

# Example: Use LM Studio locally
# LLM_PROVIDER=openai
# OPENAI_API_KEY=lm-studio
# OPENAI_BASE_URL=http://localhost:1234/v1
# OPENAI_MODEL=local-model

# Custom base URL for OpenAI-compatible APIs (leave blank for official OpenAI)
# OPENAI_BASE_URL=

# ============================================================================
# KOSMOS DATABASE CONFIGURATION (Shared with Int Crucible)
# ============================================================================
# Int Crucible shares the same database as Kosmos.
# The DATABASE_URL above is used by both systems.

# Enable database echo (SQL logging for debugging)
# DATABASE_ECHO=false

# ============================================================================
# KOSMOS LOGGING CONFIGURATION (Optional)
# ============================================================================
# Kosmos has its own logging settings that can be customized here

# Log format: json or text
# LOG_FORMAT=json

# Log file path (leave empty to log to stdout only)
# LOG_FILE=logs/kosmos.log

# Enable debug mode (verbose output)
# DEBUG_MODE=false

# ============================================================================
# ADDITIONAL KOSMOS SETTINGS (Optional, Advanced)
# ============================================================================
# For more Kosmos configuration options, see vendor/kosmos/.env.example
# Int Crucible only needs the LLM provider settings above, but you can
# customize Redis, Neo4j, Vector DB, and other Kosmos features if needed.

# Redis Cache (optional, for performance)
# REDIS_ENABLED=false
# REDIS_URL=redis://localhost:6379/0

# Vector Database (for semantic search, if needed)
# VECTOR_DB_TYPE=chromadb
# CHROMA_PERSIST_DIRECTORY=.chroma_db

# Neo4j Knowledge Graph (optional)
# NEO4J_URI=bolt://localhost:7687
# NEO4J_USER=neo4j
# NEO4J_PASSWORD=kosmos-password
